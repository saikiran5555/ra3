{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84ef3a3f",
   "metadata": {},
   "source": [
    "Ridge Regression, also known as L2 regularization, is a technique used for analyzing multiple regression data when multicollinearity is present. Multicollinearity occurs when independent variables in a regression model are correlated. This correlation can lead to unreliable and unstable estimates of regression coefficients in ordinary least squares (OLS) regression. Ridge Regression addresses this issue by adding a degree of bias to the regression estimates.\n",
    "\n",
    "Ordinary Least Squares Regression:\n",
    "OLS regression aims to find the coefficient values that minimize the sum of the squared differences between the observed and predicted values (the sum of squares of residuals).\n",
    "It works well when the independent variables are not too highly correlated with each other.\n",
    "However, in the presence of multicollinearity, OLS can produce high-variance estimates which means small changes in the model or data can lead to large changes in the coefficient estimates, making them unstable and unreliable.\n",
    "Ridge Regression:\n",
    "Ridge Regression modifies the OLS objective by adding a penalty term to the sum of squares of residuals. This penalty is the L2 norm (square of the magnitude) of the coefficient vector, multiplied by a regularization parameter, lambda (λ).\n",
    "The objective becomes to minimize: Sum of squares of residuals + λ * (Sum of squares of coefficients).\n",
    "The λ parameter controls the extent of the penalty:\n",
    "When λ = 0, Ridge Regression equals OLS Regression.\n",
    "As λ increases, the impact of the penalty increases, leading to smaller coefficients (but typically not zero).\n",
    "The main benefit is the reduction in model complexity and prevention of overfitting. The penalty term shrinks the coefficients towards zero, which stabilizes them in the presence of multicollinearity.\n",
    "Key Differences:\n",
    "Coefficient Shrinkage: Ridge Regression shrinks the coefficients towards zero, which can help in reducing model variance and handling multicollinearity, unlike OLS which can have large variances in such situations.\n",
    "\n",
    "Bias-Variance Trade-off: While OLS is an unbiased estimator, Ridge Regression introduces some bias into the estimates. However, this can lead to a lower overall mean squared error when there's multicollinearity because it balances bias and variance.\n",
    "\n",
    "Regularization Parameter: Ridge Regression includes a regularization parameter (λ) that is not present in OLS. This parameter must be chosen carefully, often using techniques like cross-validation.\n",
    "\n",
    "Zero Coefficients: Unlike Lasso Regression (another regularization technique), Ridge Regression does not set coefficients to zero (does not perform feature selection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b514bb1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
